%
% File acl2021.tex
%
%% Based on the style files for EMNLP 2020, which were
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2021}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{NLP Project Proposal: Deep Detectives}

\author{Victor Heredia \\
  \texttt{victor32@pdx.edu} \\\And
  Brandon Le \\
  \texttt{lebran@pdx.edu} \\
  Son Vu \\
  \texttt{vson@pdx.edu} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
This document contains the instructions for preparing a manuscript for the proceedings of ACL-IJCNLP 2021.
The document itself conforms to its own specifications, and is therefore an example of what your manuscript should look like.
These instructions should be used for both papers submitted for review and for final versions of accepted papers.
Authors are asked to conform to all the directions reported in this document.
\end{abstract}

\section{An Existing Research Paper Survey}

\subsection{Summary}

\paragraph{}
In the paper, Radford et al describe GPT-2, a language model (LM) used to learn natural language (NL) 
tasks without any explicit supervision. These NL tasks include question-answering, machine translation, 
reading comprehension, and summarization. GPT-2 is their largest model being a 1.5B parameter Transformer 
that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot 
setting (very few or no labeled data, generalize on datasets it hasn’t been trained on before i.e. 
classifying on the fly). The approach the authors take involves language modeling: the process of breaking 
natural language down into understandable pieces. Their speculation is that “a language model with sufficient 
capacity will begin to learn to infer and perform tasks demonstrated in natural language sequences in 
order to better predict them, regardless of their method of procurement. If a language model is able to do 
this it will be, in effect performing unsupervised multitask learning.” The model they utilized is a modified 
version of the GPT (Generative Pre-trained Transformer) platform with an order of magnitude more parameters 
and a few other modifications, including data preparation.


\subsection{Bibliographical info}

\begin{itemize}
  \item Title: Language Models are Unsupervised Multitask Learners
  \item Authors: Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever
  \item Publication venue: persagen.com
  \item Publication year: 2019
  \item URL: \url{http://www.persagen.com/files/misc/radford2019language.pdf}
\end{itemize}

\subsection{Background}

\paragraph{}
The authors mention the importance of their work as a combination of two lines of work: multitask 
learning and zero-shot methods. Their motivation for working on multitask learning is with the aim 
to provide generalization to the model so it can perform a wide range of tasks and not just specific 
tasks like other NL models. Their motivation for working with zero-shot methods, which are ones that 
require minimal or no supervised data, is to provide a solution to the issues of scaling datasets that 
is inherent with prior multitask learning techniques. By combining these two methods, the authors aim 
to provide the benefits of both in order to further improve performance for natural language tasks. 

\subsection{Summary of contributions}

\subsection{Limitations and discussion}

\subsection{Why this paper?}

\subsection{Wider research context}

\section{Proposed Project Description}

\subsection{Main goals of our project}

\subsection{NLP task we address}

\subsection{Data}

\subsection{Methods}

\subsection{Baseline}

\subsection{Evaluation}

\subsection{References}

\section*{Acknowledgments}

The acknowledgments should go immediately before the references. Do not number the acknowledgments section.
\textbf{Do not include this section when submitting your paper for review.}

\bibliographystyle{acl_natbib}
\bibliography{anthology,nlp-proposal}

%\appendix



\end{document}
